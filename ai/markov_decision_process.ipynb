{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "accurate-collins",
   "metadata": {},
   "source": [
    "# Markov Decision Process\n",
    "Suppose that the environment that our agent is in is not deterministic (which means it is stochastic).\n",
    "This means that an action that the agent take may not result in the same resultant state each time the action is taken.\n",
    "For example, an AI that is moving along a line, where it can choose to move forwards or backwards. But when he decides on an action, it has 90% chance moving as per decided, but a 5% chance to move the opposite direction, and another 5% to stay in place.\n",
    "This somewhat simulates real life as physical equipment may not always give a deterministic output.\n",
    "\n",
    "Thus, to model this problem, we can use a **Markov Decision Process**\n",
    "\n",
    "## Definition\n",
    "1. States: $S$ ($S$ is the set of states, while $s$ is a specific state)\n",
    "2. Actions: $A$ ($A$ is the set of actions, while $a$ is a specific action)\n",
    "3. Transition Model: $T(s,a)$\n",
    "    * The set of probability distribution over the states that agent will transition to upon action $a$ in state $s$\n",
    "4. Reward function\n",
    "    * The reward the agent will receive when it reaches a certain state\n",
    "    * Reward can be defined as $\\mathbb{R}$ : State → $\\mathbb{R}$,\n",
    "    * We can also define it as $\\mathbb{R}$ : State × Action → $\\mathbb{R}$ which is equivalent\n",
    "5. Initial State\n",
    "6. Goal\n",
    "7. Terminal State\n",
    "    * no action is taken after reaching this state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9032b4",
   "metadata": {},
   "source": [
    "### Example\n",
    "```\n",
    "---------------------------------\n",
    "|       |       |       |      1|\n",
    "|       |       |       |       |\n",
    "|8      |9      |10     |11     |\n",
    "---------------------------------\n",
    "|       |       |       |     -1|\n",
    "|       |       |       |       |\n",
    "|5      |6      |X      |7      |\n",
    "---------------------------------\n",
    "|       |       |       |       |\n",
    "|       |       |       |       |\n",
    "|1      |2      |3      |4      |\n",
    "---------------------------------\n",
    "```\n",
    "\n",
    "In the above diagram, the bottom left numbers are the state numbers (where X is an unreachable state), and the top right number is the reward (all unlabeled grid have a reward of -0.4 to incentivize the agent to reach in the shortest path).\n",
    "The initial state is state 1.\n",
    "The agent can only move in one of the 4 cardinal directions.\n",
    "\n",
    "The transition probability is 80% to move in the desired direction, and an equal 5% for the other direction or not moving.\n",
    "We can imagine it as a broken robot, where it performs the issued command 80% of the time, and chooses a random move otherwise.\n",
    "\n",
    "Suppose we were in a deterministic world, then the obvious **plan** would be \"UURRR\".\n",
    "However if we gave that instruction to the agent, it would succeed only with a probability of $0.8^5\\approx 32.8\\%$, which is rather poor.\n",
    "Thus, an obvious solution is to tell the agent to make a certain action when it is in a certain state.\n",
    "This is what we call a **policy**.\n",
    "This allows our agent to still try to find a path if it were to find itself in one of the states that we did not \"plan\" for.\n",
    "Thus, we wish to find a policy that maximizes the reward obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-friendship",
   "metadata": {},
   "source": [
    "### Utility of Sequence\n",
    "Suppose our agent traverses a sequence of states $s_0, \\dots$.\n",
    "A simple metric to determine the utility of the sequence is additive, where we compute $R(s_0) + R(s_1) \\dots $.\n",
    "However, since our agent can potentially navigate the problem infinitely, it means our utility could be infinite.\n",
    "This makes it hard to compare two utility if both of their utility do not converge.\n",
    "\n",
    "Thus, we use a discountive model instead, $R(s_0) + \\gamma R(s_1) + \\gamma^2 R(s_1) \\dots, \\gamma < 1 $.\n",
    "Hence, we reward future rewards less in order to ensure convergence.\n",
    "\n",
    "The best policy would be one that gives the largest reward across all possible sequences that the agent can traverse from the starting state, weighted by probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "particular-stone",
   "metadata": {},
   "source": [
    "### Inspiration\n",
    "\n",
    "Suppose we were at state 1, and deciding to go to state 5 or state 2.\n",
    "Suppose that we somehow knew the \"potential value\" of state 2 and state 5.\n",
    "From the grid, we can see that state 5 is probably \"better\" than state 2 because it is less likely to enter the right path into the terminal state of -1 (however, the agent does not know this).\n",
    "Thus, the potential value of state 5 (if we calculated it) would be higher than state 2.\n",
    "Hence, it makes sense that our agent should try to take the action that gives the higher probability to enter state 5 than state 2.\n",
    "\n",
    "We can formalize the intuition as per below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latter-darwin",
   "metadata": {},
   "source": [
    "### Formalization\n",
    "$U^\\pi$: Utility function of the policy\n",
    "\n",
    "$U^\\pi(s)$: Utility of policy at state s, or also the utility of the policy when the agent starts as s.\n",
    "\n",
    "$\\pi^∗(s)$ = best policy for state (s) = $argmax_\\pi U^\\pi(s)$\n",
    "\n",
    "$P(s_0|s, \\pi(s))$: Probability that we will get to s’ from s after taking an action from policy π \n",
    "\n",
    "\n",
    "$\\pi^*(s) = argmax Pr(s'|s, a)U^{\\pi^*}(s')$ : (The best action to take at state (s) is the one that has the highest expected utility across all possible resultant states)\n",
    "\n",
    "\n",
    "It is important to note that the optimal policy is independent of starting state, since the policy needs to map all state to an action.\n",
    "\n",
    "We denote $U(s) = U^{\\pi^*}(s)$ as shorthand for the utility of the state using the optimal policy.\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "U^\\pi(s) &= E\\left[\\sum _{t=0} ^ \\infty \\gamma^t R(S_t)\\right] & \\text{Infinite horizon} \\\\\n",
    "&=E\\left[R(S_0) + \\sum _{t=1} ^ \\infty \\gamma^t R(S_t)\\right]&\\\\\n",
    "&=R(s) + E\\left[\\sum _{t=1} ^ \\infty \\gamma^t R(S_t | S_0 = s)\\right]&\\\\\n",
    "&=R(s) + \\gamma \\sum P(s' | s, \\pi(s)) (R(s') + E\\left[\\sum _{t=2} ^ \\infty \\gamma^{t-1}  R(S_t | S_1 = s')\\right])&\\\\\n",
    "&=R(s) +\\gamma  \\sum P(s' | s, \\pi(s)) (R(s') + E\\left[\\sum _{t'=1} ^ \\infty \\gamma^{t'}  R(S_t' | S_0 = s')\\right])& \\text{(Notice that the 2 series is identical)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Thus, we get the following recurrence\n",
    "\n",
    "$U^\\pi(s) = R(s) +  \\gamma \\sum P(s' | s, \\pi(s)) U^\\pi(s')$\n",
    "\n",
    "Equivalently\n",
    "\n",
    "$U(s) = R(s) +  \\gamma \\sum P(s' | s, \\pi^*(s)) U(s')$\n",
    "\n",
    "$= R(s) + \\gamma \\max \\sum P(s' | s,a) U(s')\\text{ for a }\\in A(s)$ as the optimal policy will pick the state with the highest utility\n",
    "\n",
    "Which is similar to our inspiration.\n",
    "\n",
    "This is also known as the **Bellman Equation**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "physical-angel",
   "metadata": {},
   "source": [
    "$$U(s) = R(s) + \\gamma \\max _{a \\in A(s)} \\sum _{s'} P(s' | s, a) U(s')$$\n",
    "\n",
    "We can write expression for all the states so that we can solve the utility of every state.\n",
    "\n",
    "However, this is rather difficult because of non-linear equations (Max is non-linear)\n",
    "\n",
    "## Value Iteration\n",
    "1. Initialize utilities to some value, for instance 0.\n",
    "2. $U_i(s)$: Utility value at iteration i\n",
    "3. $U_{i+1}(s) \\leftarrow R(s) + \\gamma \\max \\sum _{s'} P(s'|s, a) U_i (s')$\n",
    "4. Repeat until insignificant change to utility\n",
    "\n",
    "It can be proven that $lim_{i \\rightarrow \\infty} U_i(s) = U(s)$, which is useful for us since we can just set some arbitrary initial value and converge towards the correct answer by iterating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c373a391",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = {\n",
    "    1: \"UR\",\n",
    "    2: \"LUR\",\n",
    "    3: \"LR\",\n",
    "    4: \"UL\",\n",
    "    5: \"URD\",\n",
    "    6: \"ULD\",\n",
    "    7: \"\",\n",
    "    8: \"RD\",\n",
    "    9: \"LDR\",\n",
    "    10: \"LR\",\n",
    "    11: \"\",\n",
    "}\n",
    "\n",
    "def next_state(state, action):\n",
    "    if action == 'U':\n",
    "        return state + 4 if state <= 2 else state + 3\n",
    "    if action == 'D':\n",
    "        return state - 4 if state <= 6 else state - 3\n",
    "    if action == 'L':\n",
    "        return state - 1\n",
    "    if action == 'R':\n",
    "        return state + 1\n",
    "\n",
    "next_states = {state: [next_state(state, a) for a in action] for state,action in actions.items()}    \n",
    "    \n",
    "T = {state: {a:[(0.8, new_state) if next_state(state, a) == new_state else (0.2/(len(next_states[state]) - 1), new_state) for new_state in next_states[state]] for a in action} for state,action in actions.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "36da23f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "R = defaultdict(int)\n",
    "R[7] = -1\n",
    "R[11] = 1\n",
    "\n",
    "states = [i for i in range(1, 12)]\n",
    "\n",
    "U = defaultdict(int)\n",
    "\n",
    "def bellman(U, states, actions, R, T, gamma=0.9):\n",
    "    U_p = defaultdict(int)\n",
    "    for s in states:\n",
    "        arr = [sum(p * U[new_s] for p, new_s in future) for future in (T[s][a] for a in actions[s])]\n",
    "\n",
    "        next_u = max(arr) if arr else 0\n",
    "        U_p[s] = R[s] + gamma * next_u\n",
    "    return U_p\n",
    "\n",
    "for _ in range(100):\n",
    "    U = bellman(U, states, actions, R, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "658f14b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'U', 2: 'U', 3: 'L', 4: 'L', 5: 'U', 6: 'U', 8: 'R', 9: 'R', 10: 'R'}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def best_action(U, actions):\n",
    "    result = {}\n",
    "    for state in U:\n",
    "        arr = [(U[next_state(state, a)], a) for a in actions[state]]\n",
    "        if not arr:\n",
    "            continue\n",
    "        result[state] = max(arr)[1]\n",
    "    return result\n",
    "        \n",
    "best_action(U, actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30efd37d",
   "metadata": {},
   "source": [
    "```\n",
    "---------------------------------\n",
    "|       |       |       |      1|\n",
    "|   >   |   >   |   >   |       |\n",
    "|8      |9      |10     |11     |\n",
    "---------------------------------\n",
    "|       |       |       |     -1|\n",
    "|   ^   |   ^   |       |       |\n",
    "|5      |6      |X      |7      |\n",
    "---------------------------------\n",
    "|       |       |       |       |\n",
    "|   ^   |   ^   |   <   |   <   |\n",
    "|1      |2      |3      |4      |\n",
    "---------------------------------\n",
    "```\n",
    "\n",
    "Hence, we get the policy above, which fits our intuition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designing-escape",
   "metadata": {},
   "source": [
    "## Policy Iteration\n",
    "\n",
    "Find $\\pi^*(s): state \\rightarrow actions$\n",
    "\n",
    "Initialize policy: $\\pi_0(s) : State \\rightarrow Action$. Arbritrary mapping\n",
    "\n",
    "We can evaluate the value of each state of that given policy as per below\n",
    "\n",
    "$$\n",
    "U^\\pi_i(s) = R(s) + \\gamma \\sum _{s'} P(s' | s, \\pi_i(s)) U^{\\pi_i}(s')\n",
    "$$\n",
    "\n",
    "If we have $n$ states, then we have $n$ linear equations\n",
    "\n",
    "we can use $O(n^3)$ Gaussian elimination\n",
    "\n",
    "Then, we update our policy\n",
    "\n",
    "$$\\pi_{i+1}(s) = argmax_{a \\in A(s)} \\sum _{s'} P(s' | s,a) U^{\\pi_i} (s')$$\n",
    "\n",
    "For a given state s, select the best action that gives the highest utility probabilistically\n",
    "\n",
    "Do until convergence\n",
    "\n",
    "$\\forall s, \\lim_{i \\rightarrow \\infty} \\pi_i(s) = \\pi^*(s)$\n",
    "\n",
    "---\n",
    "\n",
    "For all our problems up till now, we had to iterate $T(s,a)$, transition of state given an action. \n",
    "This may be too large for certain problems.\n",
    "For example, the coordinate of the robot in real life can take any real values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dca6c8",
   "metadata": {},
   "source": [
    "## Q Learning\n",
    "Instead of trying every action at every state, we learn an action at every state instead.\n",
    "\n",
    "By the Bellman Equation\n",
    "\n",
    "$$U(s) = R(s) + \\gamma \\max _{a \\in A(s)} \\sum _{s'} P(s' | s, a) U(s')$$\n",
    "\n",
    "We want to move the max outside, such that our utility function is simply:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "U(s) &= R(s) + \\gamma \\max _{a \\in A(s)} \\sum _{s'} P(s' | s, a) U(s') \\\\\n",
    "&= \\max _{a \\in A(s)}  \\left( R(s) + \\gamma \\sum _{s'} P(s' | s, a) U(s') \\right)\\\\\n",
    "&= \\max _{a \\in A(s)} Q(s,a)\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Hence, getting our (desired) definition of $Q(s,a) = R(s) + \\gamma \\sum _{s'} P(s' | s, a) U(s') $.\n",
    "\n",
    "Expanding this definition:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Q(s,a) &= R(s) + \\gamma \\sum _{s'} P(s' | s, a) U(s') \\\\\n",
    "&= R(s) + \\gamma \\sum _{s'} P(s' | s, a) \\max_{a'} Q(s',a')\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Hence, our iterative process is as follows:\n",
    "\n",
    "1. Initialize $\\hat Q_0(s,a)$\n",
    "2. Choose action a to get new state s'\n",
    "3. Update $\\hat Q_i(s,a)$\n",
    "\n",
    "for everything else, $\\hat Q_i(s', a') = \\hat Q_{i-1}(s',a')$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3436bff",
   "metadata": {},
   "source": [
    "### Choose action(s)\n",
    "* $\\hat a \\leftarrow argmax \\hat Q(s,a)$ \"best action\"\n",
    "* With probability $\\beta$, we choose $\\hat a$ else we choose a random move  or base on $\\hat Q(s,a)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba66a8b",
   "metadata": {},
   "source": [
    "### Update $\\hat Q(s,a)$\n",
    "This identity is the rewards if we take the random action $a$:\n",
    "\n",
    "$$R(s) + \\gamma \\max_{a'} Q(s', a') $$\n",
    "\n",
    "However, after some time, we want our agent to simply pick the most optimal move that it has learnt, thus our update function would be\n",
    "\n",
    "$$\\hat Q(s,a) = r( R(s) + \\gamma \\max_{a'} Q(s',a')) + (1-r) \\hat Q(s,a)$$\n",
    "\n",
    "where $r$ is the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ba48fd",
   "metadata": {},
   "source": [
    "#### Learning Rate\n",
    "When $r =1$, we get back our definition of Q\n",
    "\n",
    "When $r =0$, we stop updating our Q's\n",
    "\n",
    "Initially, we want high alpha but tapers to 0 over time\n",
    "\n",
    "Similar to [simulated annealing](./local_search#simulated-annealing.ipynb), we want to explore a lot in the early phase and less later, so we can use $\\frac{1}{t}$ for example.\n",
    "\n",
    "However, we would like the function to depend on (s,a), so we define\n",
    "\n",
    "$N(s,a)$: How many times action a was taken at state s.\n",
    "\n",
    "And we want a function $r (N(s,a))$ that is decreasing in $N(s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72c1811",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "073ffc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from copy import deepcopy\n",
    "\n",
    "Q = defaultdict(lambda :defaultdict(int))\n",
    "\n",
    "def random_action(actions, state):\n",
    "    arr = actions[state]\n",
    "    if not arr:\n",
    "        return None\n",
    "    return arr[randint(0, len(arr)-1)]\n",
    "\n",
    "def q_learning(Q, states, R, action_func, r=0.9, gamma=0.9):\n",
    "    new_Q = deepcopy(Q)\n",
    "    for state in states:\n",
    "        action = action_func(state)\n",
    "        \n",
    "        if action:\n",
    "            s_prime = next_state(state, action)\n",
    "            future_states = Q[s_prime].values()\n",
    "            best_reward = max(future_states) if future_states else 0\n",
    "        else:\n",
    "            best_reward = 0\n",
    "        new_Q[state][action] = r * (R[state] + gamma * best_reward) + (1-r) * Q[state][action]\n",
    "\n",
    "    return new_Q\n",
    "\n",
    "for i in range(10000):\n",
    "    Q = q_learning(Q, states, R, lambda s: random_action(actions, s),r=1/(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "cc90dbce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('U', 0.19607454360750393),\n",
       " ('U', 0.27901898160925537),\n",
       " ('L', 0.16644153675358755),\n",
       " ('L', 0.08908041460559306),\n",
       " ('U', 0.3219448214254002),\n",
       " ('U', 0.48120766924900515),\n",
       " (None, -1.0),\n",
       " ('R', 0.5415381022026509),\n",
       " ('R', 0.7052872671551181),\n",
       " ('R', 0.8889535139134304),\n",
       " (None, 1.0)]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[max(actions.items(), key=lambda x: x[1]) for actions in Q.values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48eb54a6",
   "metadata": {},
   "source": [
    "And indeed, we get the same decision as when we computed the utility values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a33b2cc",
   "metadata": {},
   "source": [
    "### Approximate Q-function\n",
    "\n",
    "$\\hat Q(s,a), |\\{(s,a)\\}|$ can be very large\n",
    "\n",
    "The space of (S,a) can be very large, so we want to reduce the search space\n",
    "\n",
    "Thus instead, we define Q as \n",
    "\n",
    "$Q(s,a) = \\sum ^n _{i=1} f_i(s,a) w_i$\n",
    "\n",
    "where $f_i$ are the different feature function that when given a state and action, returns a value for that feature. $w_i$ is the weight for that feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greater-ordinance",
   "metadata": {},
   "source": [
    "#### Dimensionality reduction\n",
    "\n",
    "$\\hat w_i = \\hat w_i + \\alpha [R(s) + \\gamma \\max_{a'} \\hat Q(s', a') - \\hat Q(s,a)] \\frac{\\partial \\hat Q(s,a)}{\\partial \\hat w_i} $\n",
    "\n",
    "$difference = R(s) + \\gamma \\max_{a'} \\hat Q(s', a') - \\hat Q(s,a)$\n",
    "\n",
    "$\\hat w_i = \\hat w_i + \\alpha(difference) f_i(s,a)$\n",
    "\n",
    "So the problem reduces to trying to find the correct weights for each of the feature to obtain the optimal policy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
