{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "approved-reviewer",
   "metadata": {},
   "source": [
    "# Text Classification\n",
    "A mapping h from input data $x$ to a label $y$.\n",
    "$x$ exists from some instance space $\\bf x$ and y exists in some output space $\\bf y$.\n",
    "\n",
    "We define $h(x)$ as the true mapping\n",
    "\n",
    "We wish to find best $\\hat h(x)$ that approximates $h(x)$.\n",
    "\n",
    "The following are possible methods to do so:\n",
    "* Rule Based (Decision Rules)\n",
    "* Supervised Learning\n",
    "\n",
    "**Applications**\n",
    "* Subject Classification\n",
    "* Spam Detection\n",
    "* Authorship Attribution\n",
    "* Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "musical-floating",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "A typical approach is to use a bag-of-words classifier\n",
    "\n",
    "If the classifier finds `good` or `great` then the sentiment is likely to be good.\n",
    "If the classifier finds `bad` or `terrible` then the sentiment is likely to be good.\n",
    "\n",
    "### Issues\n",
    "#### Negation\n",
    "Negation might change the sentiment expressed by a word. `not good`.\n",
    "\n",
    "Solution: prepend a special prefex to each word after a negation until the end of the sentence.\n",
    "\n",
    "`I do not like this film` $\\rightarrow$ `I do NOT_like NOT_this NOT_film`\n",
    "\n",
    "#### Ambiguity\n",
    "Some words can fall into either category based on context.\n",
    "\n",
    "* \"This movie is good for people with poor taste\"\n",
    "\n",
    "where `good` is used in a negative sense.\n",
    "\n",
    "#### Lack of Data\n",
    "Some words only appear infrequently. \n",
    "\n",
    "Solution: use offline dictionary to prepare model for analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exempt-greensboro",
   "metadata": {},
   "source": [
    "\n",
    "### tf-idf\n",
    "Supposed that we have these words $(w_1, \\dots, w_V)$, \n",
    "then for each collection, we obtain a count vector  $(c(w_1), \\dots, c(w_V))$, which shows the count of each word in the collection.\n",
    "Using these counts, we can use it as an indicator as to which class each document belongs to.\n",
    "\n",
    "#### Term frequency\n",
    "Definition: $tf_{w,d}$ of a **word** $w$ in **document** $d$ is the number of times $w$ occurs in $d$.\n",
    "\n",
    "It makes sense that if a word appears many times in a document, the word is likely to characterise the document.\n",
    "However, the frequency may not be a linear relationship, where a document with 100 occurence of the word is not 100 times more relevant compared to a document with 1 occurence of the word. \n",
    "Thus, we take the log frequency instead, to taper off the effects of high occurence of a word.\n",
    "\n",
    "Hence, we derive that the weight of a word in d is\n",
    "\n",
    "$$\n",
    "w_{d} = \n",
    "    \\begin{cases}\n",
    "      1 + \\log tf_{w, d} & tf_{w, d} > 0 \\\\\n",
    "      0 & \\text{otherwise} \\\\\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "#### Document frequency\n",
    "Definition: $df_{w}$ of a **word** $w$ is the number of documents that contains $w$.\n",
    "\n",
    "Notice that for words that appears in many documents are less likely to be as descriptive as words that appears in few documents.\n",
    "\n",
    "For example, `the` is less likely to differentiate 2 documents compared to `astigmatism`.\n",
    "\n",
    "Thus, we obtain the relationship that the higher the document frequency, the lower the word's significance.\n",
    "\n",
    "Thus, the inverse document frequency ($idf$), is defined as:\n",
    "\n",
    "$$\n",
    "idf_w = \\log(N/df_w)\n",
    "$$\n",
    "\n",
    "Again, the log scale is used to dampen the effects of high document frequency.\n",
    "\n",
    "#### Collection Frequency\n",
    "Collection frequency is the number of occurence of a word in the collection, counting multiple occurence.\n",
    "\n",
    "Contrast this with document frequency, which is the number of documents where a word appears in.\n",
    "\n",
    "Notice that document frequency would be generally more indicative of the significance of the word since the same word appearing in the same document multiple times should not reduce the importance of the word.\n",
    "\n",
    "For example, if you have many English documents and a single math document about geometry, even though `triangle` may appear many times in the collection, it is still a good differentiator for English and math document.\n",
    "\n",
    "#### tf-idf weighting\n",
    "Thus, it brings us to the step of bringing the two metrics together.\n",
    "\n",
    "A typical way to do this is:\n",
    "$$\n",
    "weight_{w,d} = (1+\\log tf_{w,d}) \\times \\log(N / df_w)\n",
    "$$\n",
    "\n",
    "Notice that it follows our intuition that the term frequency in the document and the term rarity across the document both have a positive effect on the weight.\n",
    "\n",
    "### Weight Matrix\n",
    "Thus, we can now replace our count matrix with the weight matrix, each with its own $tf-idf$ weight value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprised-invalid",
   "metadata": {},
   "source": [
    "### Vector Space Model\n",
    "We now have a $|V|$-dimentional vector space, where each cardinal direction in the vector correspond to a word.\n",
    "\n",
    "Words are the axes of the space.\n",
    "\n",
    "Documents are points or vectors in the space.\n",
    "\n",
    "We will now devise methods to compare the 2 documents.\n",
    "\n",
    "#### Geometry\n",
    "From linear algebra, we know that the dot product gives us an indication of how close 2 vectors are.\n",
    "\n",
    "$$\n",
    "|\\vec a \\cdot \\vec b| = |\\vec a | |\\vec b| \\cos \\theta\n",
    "$$\n",
    "\n",
    "If the 2 vectors are similar, then the resultant vector will have large.\n",
    "\n",
    "However, a large vector (caused by a frequent word) can also lead a large resultant vector.\n",
    "\n",
    "Thus, it is better to use the angle, which normalizes the magnitude of the 2 vectors.\n",
    "\n",
    "\n",
    "$$\n",
    "\\cos \\theta = \\frac{|\\vec a \\cdot \\vec b|}{|\\vec a | |\\vec b|} = \\frac {\\sum ab} {\\sqrt{\\sum a^2 \\sum b^2}}\n",
    "$$\n",
    "\n",
    "And since every entry in our vector is positive, this means the resultant angle have the range $0 \\leq \\theta \\leq 1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-horse",
   "metadata": {},
   "source": [
    "### Naive Bayes Model <a id=naive-bayes></a>\n",
    "Naive Bayes model is depends on a simple representation of the document, the Bag of Words.\n",
    "\n",
    "It is based on the premise that a document of a certain class is more likely to produce words from a certain vocabulary.\n",
    "\n",
    "Thus, for a given document $W$, we wish to find the class y which gives the largest $Pr(y | W)$\n",
    "\n",
    "By Bayes Rule, we can expand the probability as follows\n",
    "\n",
    "$$\n",
    "Pr(y | W) = \\frac{Pr(W | y) Pr(y)}{Pr(W)}\n",
    "$$\n",
    "\n",
    "We can further simplify the expression since we are finding the best class y.\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{argmax} Pr(y | W) = \\text{argmax} \\frac{Pr(W | y) Pr(y)}{Pr(W)} = \\text{argmax} Pr(W | y) Pr(y)\n",
    "$$\n",
    "\n",
    "Now, we make the assumption that \n",
    "* Each word's probability to appear does not depend on its location in the document\n",
    "* All the events of any word appearing are independant of each other\n",
    "\n",
    "Thus, the probabilty of all the word appearing in the document given the class would be simply the product of all the probabilties of each word.\n",
    "$$\n",
    "\\text{argmax} Pr(y | W) = \\text{argmax} Pr(W | y) Pr(y) =  \\text{argmax} \\prod _{i=1} ^ n Pr(w_i | y) Pr(y)\n",
    "$$\n",
    "\n",
    "Note that this is similar to treating this as a unigram language model.\n",
    "\n",
    "#### MLE Estimations\n",
    "We can estimate each of the probabilities as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Pr(w_i | y) &= \\frac{c(w_i, y)} {\\sum _{w \\in V} c(w, y)} &\\\\\n",
    "Pr(y) &= \\frac{N_y}{N_{doc}} & N_y = \\text{ Number of document with class of y}\\\\\n",
    "& & N_{doc} = \\text{ total number of document}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "As with language model, note that we might run into issues of [underflow](./language_model.ipynb#underflow) or [unknown words](./language_model.ipynb#unknown_words), both of which can be solved the same way as discussed in the language model chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "regulated-steering",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from math import log, exp\n",
    "class NaiveBayes:\n",
    "    def __init__(self, classes, add_one=True, smooth_prior=True):\n",
    "        self.add_one = add_one\n",
    "        self.vocab = set()\n",
    "        self.table = {cls:Counter() for cls in classes}\n",
    "        self.documents = Counter()\n",
    "        self.smooth_prior = smooth_prior\n",
    "        \n",
    "    def add_document(self, tokens, label):\n",
    "        self.vocab.update(tokens)\n",
    "        self.table[label].update(tokens)\n",
    "        self.documents.update([label])\n",
    "        \n",
    "    def get_prob(self, tokens, label):\n",
    "        prob = 0\n",
    "        for token in tokens:\n",
    "            if self.add_one:\n",
    "                num = self.table[label][token]+1\n",
    "                dem = sum(self.table[label].values()) + len(self.vocab)\n",
    "            else:\n",
    "                num = self.table[label][token]\n",
    "                dem = sum(self.table[label].values())\n",
    "\n",
    "            print(token, num, dem)\n",
    "                \n",
    "            if num == 0:\n",
    "                return 0\n",
    "            prob += log(num) - log(dem)\n",
    "        \n",
    "        if self.smooth_prior:\n",
    "            prior = (self.documents[label] + 1) / (sum(self.documents.values()) + len(self.table.keys()))\n",
    "        else:\n",
    "            prior = self.documents[label]  / sum(self.documents.values())\n",
    "        \n",
    "        return exp(prob + log(prior))\n",
    "\n",
    "    def predict(self, tokens):\n",
    "        return sorted([(self.get_prob(tokens, label), label) for label in self.table.keys()])[-1][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-frost",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "After producing a classification model, we need to evaluate its performance.\n",
    "\n",
    "A typical way is to use a confusion matrix\n",
    "\n",
    "|                    | Actual Positive | Actual Negative |\n",
    "| ------------------ | --------------- | --------------- |\n",
    "| Predicted Positive | True Positive   | False Positive  |\n",
    "| Predicted Negative | False Negative  | True Negative   |\n",
    "\n",
    "### Accuracy\n",
    "A natural metric is accuracy, which is simply $\\frac{TP + TN}{TP+TN+FP+TN}$.\n",
    "\n",
    "Accuracy indicates how many of the sample the model gets correct.\n",
    "\n",
    "However, we do not usually use this to evaluate a classification model, because it does not work well for unbalanced samples.\n",
    "\n",
    "Suppose our samples consists of 90% positive documents and 10% negative documents.\n",
    "\n",
    "If our model simply classifies everything as positive, it will get an accuracy of 90%, but this is not indicative of its inability to differentiate the two classes.\n",
    "\n",
    "### Precision\n",
    "Precision is defined as $\\frac{TP}{TP+FP}$.\n",
    "\n",
    "It is indicative of the correctness of positive predictions.\n",
    "\n",
    "Precision is concerned about the following row of the table\n",
    "\n",
    "\n",
    "|                    | Actual Positive | Actual Negative |\n",
    "| ------------------ |:---------------:|:---------------:|\n",
    "| Predicted Positive | **True Positive**   | **False Positive**  |\n",
    "| Predicted Negative | -  | -   |\n",
    "\n",
    "### Recall\n",
    "Recall is defined as $\\frac{TP}{TP+FN}$.\n",
    "\n",
    "It is indicative of the how many of the positive cases that the model is able to identify.\n",
    "\n",
    "Recall is concerned about the following row of the table\n",
    "\n",
    "\n",
    "|                    | Actual Positive | Actual Negative |\n",
    "| ------------------ |:---------------:|:---------------:|\n",
    "| Predicted Positive | **True Positive**   | - |\n",
    "| Predicted Negative | **False Negative**  | -   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satellite-heater",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "Consider a classifier which we can tune its strictness in classifying.\n",
    "\n",
    "If we increase the strictness, it will predict less of the samples as positive, thus it will begin to only predict positive for those samples that it is very confident about.\n",
    "\n",
    "Thus, precision would increase and recall would fall.\n",
    "\n",
    "We can see the reverse happening if we were to reduce strictness.\n",
    "\n",
    "Thus, there is a general trend of inverse relationship between recall and precision\n",
    "\n",
    "### Combined Measure\n",
    "Because we wish to account for both precision and recall for the evaluation of our classifier, we can define a measure F as such:\n",
    "\n",
    "$$\n",
    "F = \\frac{2pr}{p+r}\n",
    "$$\n",
    "\n",
    "This is also known as the geometric mean of $p$ and $r$.\n",
    "\n",
    "Note that compared to the arithmetic mean, defined as $\\frac{p+r}{2}$, the geometric mean penalizes the classifier heavier for having either one of the $p$ or $r$ being small."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
