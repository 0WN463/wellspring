{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular Expressions\n",
    "\n",
    "A formal language for specifying a set of text strings.\n",
    "\n",
    "A pattern that specify text search string to find in a corpus of text.\n",
    "\n",
    "For more info, refer to [Regular Expressions](../general_computing/regular_expression.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Errors\n",
    "\n",
    "1. False positive (Type I): Matching strings that we do not want to be matched\n",
    "2. False negative (Type II): Not matching strings we want to match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that we wish to find all occurence of `at`, regardless of capitalization, in the following corpus\n",
    "\n",
    "`All cats like to sleep at home. At noon, they wake up to eat`\n",
    "\n",
    "If we use the regular expression of `at`, we will get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All c\u001b[31mat\u001b[0ms like to sleep \u001b[31mat\u001b[0m home. At noon, they wake up to e\u001b[31mat\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def show_matches(exp, text):\n",
    "    for m in list(re.finditer(exp, text))[::-1]:\n",
    "        a, b = m.start(), m.end()\n",
    "        text = text[:b] + \"\\x1b[0m\" + text[b:]\n",
    "        text = text[:a] + \"\\x1b[31m\" + text[a:]\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "corpus = \"All cats like to sleep at home. At noon, they wake up to eat\"\n",
    "\n",
    "print(show_matches(r\"at\", corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which has both false positive when it matches `c(at)s` and `e(at)`, and false negative with `At`.\n",
    "\n",
    "The more correct expression would be `\\w[Aa][Tt]\\w`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All \u001b[31mcats\u001b[0m like to sleep at home. At noon, they wake up to eat\n"
     ]
    }
   ],
   "source": [
    "print(show_matches(r\"\\w[Aa][Tt]\\w\", corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing errors involves two opposing efforts\n",
    "* Increase **accuracy/precision** $\\rightarrow$ Minimise false positive\n",
    "* Increase **coverage/recall** $\\rightarrow$ Minimise false negatives\n",
    "\n",
    "One can see that if we increase the strictness of our match, it is likely that we improve precision but reduce recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus Preprocessing\n",
    "To determine how many words there are in a corpus, we need to define what a unique word is. \n",
    "Words can be defined by their:\n",
    "\n",
    "* Lemma: same stem, part of speech, rough word sense \n",
    "\n",
    "    * cat and cats = same lemma\n",
    "\n",
    "* Wordform: the full inflected surface form\n",
    "\n",
    "    * cat and cats = different wordforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type: Element of the vocabulary\n",
    "\n",
    "Token: Instance of that type in running text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issues with Tokenization\n",
    "Tokenization in English have many ambiguities:\n",
    "\n",
    "* Apostrophe used for belonging may split into different word forms\n",
    "    * London's $\\rightarrow$ London, Londons, London's\n",
    "* Apostrophe used for contraction may cause odd word fragments\n",
    "    * what're $\\rightarrow$ what + re\n",
    "* Compound words no longer become a single word\n",
    "    * break-through $\\rightarrow$ breakthrough\n",
    "* Acronymns with perdios may cause issues:\n",
    "    * L.A.P.D, Mr.\n",
    "    \n",
    "    \n",
    "This permeates through other languages as well\n",
    "\n",
    "* French: L'ensemble\n",
    "\n",
    "* German: Lebensversicherungsgesellschaftsangestellter\n",
    "\n",
    "    * Where it is composed of compound words. We will need a scheme to split it into its components\n",
    "\n",
    "* Chinese/Japanese have no spaces between words\n",
    "\n",
    "    * Multiple syllabaries are intermingled in Japanese: Hiragana, Katagana, Kanji\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Tokenization in Chinese\n",
    "\n",
    "Standard baseline segmentation algorithm: \n",
    "\n",
    "* Maximum/Greedy Matching\n",
    "\n",
    "### Maximum Matching\n",
    "1. Start at the beginning of the string\n",
    "2. Find the longest word in dictionary that matches the string\n",
    "3. Move the pointer over the word in string\n",
    "4. Repeat 2-4\n",
    "\n",
    "This generally doesn't work in English\n",
    "\n",
    "`Themeterratedropped` may be tokenized to \"Theme terra ted ropped\", when \"The meter rate dropped\" may be the intended meaning.\n",
    "\n",
    "However, this works rather well for Chinese"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Morphology <a id=\"morphology\"></a>\n",
    "**The study of the way words are build from morphemes**\n",
    "\n",
    "Morphemes: The minimal meaning-beraing unit in a language\n",
    "* Stems: Core meaning-bearing unit (eg. cow)\n",
    "* Affixes: Bits that adhere to stems. Often carry grammatical function (eg. -s)\n",
    "\n",
    "### Benefits of Morphology\n",
    "\n",
    "It is inefficient to listing all the different morphological variants of a word in a dictionary.\n",
    "For example, storing `ride`, `riding`, `rides`, `rode` ... will take up excessive space.\n",
    "\n",
    "Thus, affixes are productive, as we can simply generate new words from its stem.\n",
    "\n",
    "Also, it might be impossible to list all morphological variants of every word, especially for morphologically complex languages like Turkish.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forms of Morphology\n",
    "\n",
    "#### Inflectional\n",
    "* Combine a stem and an affix to form a word in the same class as stem\n",
    "    * ie noun $\\rightarrow$ noun\n",
    "* For syntactic function like agreement\n",
    "* e.g., -s to form plural form of a noun\n",
    "\n",
    "#### Derivational\n",
    "* Combine a stem and an affix to form a word in a different class\n",
    "    * ie verb $\\rightarrow$ noun\n",
    "* Harder to predict the meaning of the derived form\n",
    "* e.g., -ation in organize and organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out of Vocabulary\n",
    "New words are constantly being created.\n",
    "\n",
    "However, we can apply morphology analysis to try to understand them even though we have never seen them before.\n",
    "\n",
    "For example, even if we are in the year before \"selfie\" was first coined, we can derive that it have something to do with \"self\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repurposed Byte Pair Encoding (BPE)\n",
    "\n",
    "1. Find most frequent pairs in the string\n",
    "2. Update vocabulary with the pair with a new symbol\n",
    "3. Replace all pairs in the string with the new symbol\n",
    "4. Repeat up to k times, where k is a tunable parameter (corpus size dependant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('CzCwy', {'A': 'ww', 'B': 'Aw', 'C': 'Bx'})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def bpe(text, k):\n",
    "    text = text.lower()\n",
    "    vocab = {}\n",
    "    curr_token = \"A\"\n",
    "    for _ in range(k):\n",
    "        counter = Counter()\n",
    "        for pair in [text[i : i + 2] for i in range(len(text) - 1)]:\n",
    "            counter.update([pair])\n",
    "        most_common_pair, freq = counter.most_common(1)[0]\n",
    "        vocab[curr_token] = most_common_pair\n",
    "        text = text.replace(most_common_pair, curr_token)\n",
    "        curr_token = chr(ord(curr_token) + 1)\n",
    "\n",
    "    return text, vocab\n",
    "\n",
    "\n",
    "bpe(\"wwwxzwwwxwy\", 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "Convert text to a convenient standard form.\n",
    "\n",
    "* `U.S.A` to `USA`\n",
    "\n",
    "Alternative: asymmetric expansion\n",
    "\n",
    "* `beat` to `beat`,`beats`\n",
    "\n",
    "* `beats` to `beat`, `beats`, `Beats`\n",
    "\n",
    "* `Beats` to `Beats`\n",
    "\n",
    "Notice that `beats` $\\rightarrow$ `Beats` but `Beats` $\\not \\rightarrow$ `beats`, because \"Beats\" is likely referring to the brand \"Beats\".\n",
    "\n",
    "### Case folding\n",
    "Also known as using only upper/lower case for analysis.\n",
    "\n",
    "Folding allows us to group the same tokens that are in different part of the sentence together.\n",
    "\n",
    "`He jogs in the morning. Jogs in the morning are tiring.`\n",
    "\n",
    "where both \"jogs\" are tokenized to the same token.\n",
    "\n",
    "However, case folding may cause improper grouping of tokens also.\n",
    "\n",
    "For example, `No one can divide us! US (United States) will stand united!`, where \"us\" has a different meaning from \"US\".\n",
    "\n",
    "### Lemmatization\n",
    "Reduce words to their base form\n",
    "\n",
    "* am, are, is, were, was $\\rightarrow$ be\n",
    "* cow, cows, cow's $\\rightarrow$ cow\n",
    "\n",
    "Requires a dictionary of words with all its possible forms\n",
    "\n",
    "### Penn Treebank Tokenization\n",
    "* Seperate out clitics (morpheme that has the role of a independant word, but does not occur alone)\n",
    "\n",
    "`don't` $\\rightarrow$ `do n't`\n",
    "\n",
    "* Keep hyphenated words together\n",
    "\n",
    "* Separate out all punctuation symbols\n",
    "\n",
    "### Stemming\n",
    "Reduce terms to their stems\n",
    "\n",
    "Crude chopping of affixes\n",
    "\n",
    "`Activate, activated, activation` $\\rightarrow$ `activat`\n",
    "\n",
    "* Language dependant\n",
    "\n",
    "#### Porter Stemmer\n",
    "Efficient stemming algorithm that is build on regular expressions\n",
    "\n",
    "As series of rules that update the text in each pass.\n",
    "\n",
    "Does not require a lexicon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Segmentation\n",
    "`!` and `?` are generally unambiguous as the end of the sentence.\n",
    "\n",
    "However, `.` is ambiguous, in the case of `U.S.A`, `Dr.`,  `4.3`.\n",
    "\n",
    "To solve this, we can use a decision tree to determine if it is the end of the sentence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spelling Errors\n",
    "\n",
    "1. Non-word error detection\n",
    "    * Detecting that `desret` is not a word\n",
    "2. Isolated-word error correction\n",
    "    * Seeing that `desret` may be a mispelling of `desert`\n",
    "3. Context-sensitive error detection and correction\n",
    "    * Noticing that the context is about sweets and thus `dessert` is more fitting\n",
    "\n",
    "### Spelling Error Pattterns <a id=\"single_error\"></a>\n",
    "Single-errors are the most common error in typewritten text\n",
    "\n",
    "Types of single-errors:\n",
    "* Insertion\n",
    "    * `desserrt`\n",
    "* Deletion\n",
    "    * `dssert`\n",
    "* Substitution\n",
    "    * `dessett`\n",
    "* Transposition\n",
    "    * `dessret`\n",
    "\n",
    "### Candidate Generation\n",
    "Similar spelling\n",
    "* Small edit distance\n",
    "\n",
    "Similar pronounciation\n",
    "* Small edit distance to the pronounced word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noisy Channel Intuition\n",
    "\n",
    "We can model the corpus generation as through a noisy channel\n",
    "\n",
    "Our mission is to decode the noisy signal back into its original intentions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noisy Channel\n",
    "\n",
    "Given x as the observation of a misspelled word, we wish to find the correct word w\n",
    "\n",
    "$$\\hat w = \\text{argmax} _{w\\in V} Pr(w|x)$$\n",
    "\n",
    "$$\\hat w = \\text{argmax} _{w\\in V} \\frac{Pr(x| w)P(w)}{Pr(x)}$$\n",
    "\n",
    "Since all the $Pr(x)$ would be the same for all possible word w, we simply need to compare:\n",
    "\n",
    "$$\\hat w = \\text{argmax} _{w\\in V} Pr(x| w)P(w)$$\n",
    "\n",
    "#### Estimating $Pr(w)$\n",
    "\n",
    "We can use a Maximum Likelihood Estimate (MLE):\n",
    "\n",
    "$Pr(w) = \\frac{freq(w)}{N}$\n",
    "\n",
    "This represents the probability of the word $w$ appearing in the text\n",
    "\n",
    "#### Estimating $Pr(x|w)$  <a id=\"confusion_matrix\"></a>\n",
    "\n",
    "We can use a pre-computed confusion matrix of spelling errors to determine the probabilty that $w$ will be misspelled to $x$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edit Distance\n",
    "\n",
    "A method to compare words based on their similarity in spelling.\n",
    "\n",
    "### Uses of Edit Distance\n",
    "* Candidate Generation\n",
    "    * Most of the errors have edit distance 1; almost every error are in edit distance of 2.\n",
    "    * Allow insertion of space/hyphens\n",
    "\n",
    "* Evaluating Machine Translation and Speech Recognition\n",
    "* Named Entity Extraction and Entity Coreference\n",
    "* Nucleotide Comparison in Computation Biology\n",
    "\n",
    "The allowed edit operations are the same as those in [single errors](#single_error):\n",
    "* Insertion\n",
    "* Deletion\n",
    "* Substitution\n",
    "* Transposition\n",
    "\n",
    "However, we can assign different cost to each of the operations.\n",
    "\n",
    "For minimum edit distance (**Levenshtein distance**), insertion and deletion have a cost of 1 while substitution have a cost of 2. We do not allow for transposition for Levenshtein distance.\n",
    "\n",
    "The solution to this is the same as the variant discussed in [Dynamic Programming](../algorithm_analysis/dynamic_programming.ipynb#edit_distance) section.\n",
    "\n",
    "### Backtrace\n",
    "By using the remembering which cell we came from, we can track back the path to get the alignment of the 2 string.\n",
    "\n",
    "### Complexity\n",
    "Time: O(mn)\n",
    "\n",
    "Space: O(mn)\n",
    "\n",
    "Backtrace: O(m+n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Min Edit Distance\n",
    "We can add weights to certain edit operations, because some operations may be more common than others.\n",
    "\n",
    "In the case of spelling correction, some letters are more likely to be mistyped than others (due to proximity on the keyboard etc)\n",
    "\n",
    "In biology, certain deletion/insertion of sequences may be more common than others.\n",
    "\n",
    "To account for this, we can use the same [confusion matrix](#confusion_matrix) we used to determine the word from a noisy channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
