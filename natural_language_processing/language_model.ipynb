{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model\n",
    "\n",
    "Language models assigns probabilities to a sentence.\n",
    "\n",
    "Probability of a sequence of words: $Pr(W) = Pr(w_1, w_2, \\dots w_n)$\n",
    "\n",
    "* $Pr(\\text{Who is John Doe})$\n",
    "\n",
    "Probability of upcoming word: $Pr(w_n | w_1, w_2, \\dots w_{n-1})$\n",
    "\n",
    "* $Pr(\\text{Who} | \\text{is John Doe})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application\n",
    "* Spelling Correction\n",
    "* Speech recognition\n",
    "* Grammatical Error Correction\n",
    "* Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n-grams\n",
    "A n-gram is a sequence of n items (usually words) in the given text.\n",
    "\n",
    "## Chain rule\n",
    "By chain rule of probability, we know that\n",
    "\n",
    "$$\n",
    "Pr(A_1, \\dots , A_N) = Pr(A_1) \\times Pr(A_2 | A_1) \\dots Pr(A_N | A_{1:N-1}) = \\prod ^ N _{i=1} Pr (A_i | A_{1:i-1})\n",
    "$$\n",
    "\n",
    "where $i:j$ represents a sequence \n",
    "\n",
    "We can estimate each $Pr (A_i | A_{1:i-1})$ using Maximum Likelihood Estimate (MLE):\n",
    "\n",
    "$$\n",
    "Pr (A_i | A_{1:i-1})= \\frac{Pr (A_{1:i}) \\cap Pr(A_i)}{A_{1:i-1}} = \\frac{\\text{Count($A_{1:i}$)}} {\\text{Count($A_{1:i-1}$)}}\n",
    "$$\n",
    "\n",
    "Thus, to compute $Pr(\\text{Who is John Doe})$ \n",
    "\n",
    "$= Pr(\\text{Who}) \\times Pr(\\text{is|Who}) \\times Pr(\\text{John| who is}) \\times Pr(\\text{Doe | who is Doe})$\n",
    "\n",
    "And we estimate $Pr(\\text{is|Who}) \\approx \\frac{\\text{Count(Who is)}} {\\text{Count(Who)}}$\n",
    "\n",
    "$Pr(\\text{John |Who is}) \\approx \\frac{\\text{Count(Who is John)}} {\\text{Count(Who is)}}$\n",
    "\n",
    "So on and so forth...\n",
    "\n",
    "### Issues\n",
    "However, consider $\\frac{\\text{Count($A_{1:i}$)}} {\\text{Count($A_{1:i-1}$)}}$ where the sentence is much longer.\n",
    "\n",
    "A long sentence will cause our joint probability table to be very large.\n",
    "\n",
    "Worse still, if the sequence that we wish to predict is not seen before (which is very likely once the sentence is long), then our count in the numerator is 0.\n",
    "This leads to all our possible predictions to yield 0 probability!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Assumption\n",
    "Approximate probability of the sequence of word by assuming that they are only dependent on the **last k words**.\n",
    "\n",
    "$$\n",
    "Pr(A_1, \\dots , A_N) \\approx  \\prod ^ N _{i=1} Pr (A_i | A_{i-k:i-1})\n",
    "$$\n",
    "\n",
    "### n-Gram model\n",
    "Thus, given a n-gram model, we will approximate the next word by using the n-1 preceding words.\n",
    "\n",
    "Unigram $\\rightarrow$ 0 preceding word (Purely based on word frequency)\n",
    "\n",
    "Bigram $\\rightarrow$ 1 preceding word (Based on word frequency given previous word)\n",
    "\n",
    "Trigram $\\rightarrow$ 2 preceding word (Based on word frequency given previous 2 word)\n",
    "\n",
    "### MLE on n-grams\n",
    "Suppose we wish to estimate the MLE of $Pr(A_i | A_{i-k:i-1})$.\n",
    "\n",
    "We simply count the times that the desired n-gram appears, and divide by the count of the times that $A_{i-k:i-1} w$ appears for every possible word $w$ in the corpus. \n",
    "\n",
    "$$\n",
    "Pr(A_i | A_{i-k:i-1}) = \\frac{\\text{Count($A_{i-k:i}$)}} {\\sum _ w\\text{Count($A_{i-k:i-1} w$)}} = \\frac{\\text{Count($A_{i-k:i}$)}} {\\text{Count($A_{i-k:i-1}$)}}\n",
    "$$\n",
    "\n",
    "Where the simplification comes from realizing that the denominator is simply the count of our preceding k-gram in the corpus.\n",
    "\n",
    "Since the left side is giving the probability of $A_i$ appearing after a sequence of k words, setting $N = k+1$, we get\n",
    "\n",
    "\n",
    "$$\n",
    "Pr(A_i | A_{i-N+1:i-1}) = \\frac{\\text{Count($A_{i-N+1:i}$)}} {\\text{Count($A_{i-N+1:i-1}$)}}\n",
    "$$\n",
    "\n",
    "### Sentence Marking\n",
    "For the model to be more robust, we can allocate a special word/delimiter for sentence start and end. \n",
    "This allows us to differentiate words that are likely to start or end a sentence.\n",
    "\n",
    "More importantly, it ensures that sentences of different length will draw from the same probability distribution, which means that the sum of probability of all sentences will sum to one.\n",
    "### Underflow <a id=\"underflow\"></a>\n",
    "Multiply the probability may lead to a very small number, which may cause the arithmetic to underflow.\n",
    "\n",
    "Using the fact that \n",
    "$$P_1 \\times P_2 \\times\\dots \\times P_n \\propto \\log P_1 + \\log P_2 +\\dots+ \\log P_n$$\n",
    "\n",
    "We can calculate the sum of the logs instead, where we get a negative sum instead of a term that approaches 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unknown Words <a id=\"unknown_words\"></a>\n",
    "### Closed Vocabulary\n",
    "* Vocabulary is fixed\n",
    "* All dataset contains only words from this dictionary\n",
    "* No unknown words\n",
    "\n",
    "### Open Vocabulary\n",
    "* Test set may contain outside of vocabulary words (OOV)\n",
    "\n",
    "This may lead to counts of certain words being 0, leading to 0 probability again!\n",
    "\n",
    "## Handling Unknown Words\n",
    "#### Substitution\n",
    "1. Decide on a vocabulary list\n",
    "2. Convert all unknown words to a special token `<UNK>` during normalization.\n",
    "3. Treat the token as a regular word\n",
    "\n",
    "### Subword Morphological Processing\n",
    "Refer to [Morphology](./words.ipynb#morphology)\n",
    "\n",
    "### Smoothing\n",
    "Since the main issue is trying to predict a word that we have never seen before, the simple solution is to give a non-zero probability to OOV words.\n",
    "\n",
    "Smoothing is the act of reallocating the probabilities of n-gram such that all is non-zero.\n",
    "\n",
    "Discounting is similar, which is reallocating the count of the n-gram such that all is non-zero.\n",
    "\n",
    "#### Laplace Smoothing\n",
    "Simply add 1 to all n-gram counts.\n",
    "\n",
    "\n",
    "For bigrams, it will be\n",
    "\n",
    "$$\n",
    "Pr(w_n | w_{n-1}) = \\frac{C(w_{n-1}w_n) + 1} {\\sum _w (C(w_{n-1}w) + 1)}= \\frac{C(w_{n-1}w_n) + 1} {C(w_{n-1}) + V}\n",
    "$$\n",
    "\n",
    "#### Laplace Discounted Count\n",
    "Replace all n-gram counts with a discounted count.\n",
    "\n",
    "For bigrams, it will be \n",
    "\n",
    "$$\n",
    "Pr(w_n | w_{n-1}) = \\frac{C(w_{n-1}w_n) + 1} {C(w_{n-1}) + V} = \n",
    "\\frac{C^*(w_{n-1}w_n)}{C(w_{n-1})}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Rightarrow C^*(w_{n-1}w_n) = C(w_{n-1}) \\times \\frac{C(w_{n-1}w_n) + 1} {C(w_{n-1}) + V}\n",
    "$$\n",
    "where $C^*(w_{n-1}w_n)$ is the new discounted count for each bigram.\n",
    "\n",
    "#### Laplace Discount\n",
    "Multiply all ratio by a factor $d_c$\n",
    "\n",
    "$$\n",
    "d_c = \\frac{C^*(w_{n-1}w_n)}{C^(w_{n-1}w_n)} = \\frac{C(w_{n-1})}{C(w_{n-1}w_n)} \\times \\frac{C(w_{n-1}w_n) + 1} {C(w_{n-1}) + V}\n",
    "$$\n",
    "\n",
    "#### Add-k Smoothing\n",
    "Add k to all n-gram counts.\n",
    "\n",
    "This is a generalization of Laplace smoothing\n",
    "\n",
    "For bigrams, it will be\n",
    "\n",
    "$$\n",
    "Pr(w_n | w_{n-1}) = \\frac{C(w_{n-1}w_n) + k} {C(w_{n-1}) + kV}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backoff\n",
    "Suppose we have a n-gram model.\n",
    "\n",
    "If using a n-gram yields 0 probability for a certain sentence, we can use (n-1)-gram for that sentence instead.\n",
    "\n",
    "We use repeatedly smaller k-gram until we find that the sentence can be predicted.\n",
    "\n",
    "### Interpolation\n",
    "Use different n-grams probability estimates as metric instead.\n",
    "\n",
    "For example, combining bigram and unigram:\n",
    "\n",
    "$$\n",
    "Pr(w_1 | w_0) = \\lambda_1 P(w_1 | w_0) +  \\lambda_2 P(w_1)\n",
    "$$\n",
    "\n",
    "Where $\\sum \\lambda _i = 1$\n",
    "\n",
    "### Kneser-Ney Smoothing\n",
    "Researchers analyzed a corpus and counted the different bigrams in the training set and compared to those in the testing set.\n",
    "They found out that generally, the number of each bigram in the testing set is consistently about 0.75 less than that in the training set, for bigrams that appear more than once.\n",
    "\n",
    "Thus, we get the following smoothing method:\n",
    "\n",
    "Discount seen n-grams counts by a fixed amount and distribute it to the unseen n-grams.\n",
    "\n",
    "$$\n",
    "Pr_{seen}(w_n | w_{n-1}) = \\frac{C(w_{n-1} w_n) - \\delta}{C(w_{n-1})}\n",
    "$$\n",
    "\n",
    "For bigrams, $\\delta = 0.75$ usually set.\n",
    "\n",
    "\n",
    "$$\n",
    "Pr_{unseen}(w_n | w_{n-1}) = \\lambda \\times \\frac{\\text{Number of bigram types that ends with $w_n$}}{\\text{Number of seen bigram types}}\n",
    "$$\n",
    "\n",
    "Where $\\lambda$ is the interpolation factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "A reasonable model should assign higher probabilities to more frequent sentences and lower probabilities to rarer sentences.\n",
    "\n",
    "\n",
    "### Types of Evaluation\n",
    "* Intrinsic Evaluation\n",
    "    * Use a intrinsic metric to evaluate the model (ie **perplexity**)\n",
    "    * Easier and faster\n",
    "\n",
    "* Extrinsic Evaluation\n",
    "    * Use a downstream task\n",
    "    * Expensive and slower\n",
    "\n",
    "#### Intrinsic Evaluation\n",
    "1. Train the model on a training set\n",
    "2. Tune the parameters using a development set\n",
    "3. Test the model on a test set. Use a evaluation metric to assess performance.\n",
    "\n",
    "Common data breakdown:\n",
    "* 80% for training\n",
    "* 10% for development\n",
    "* 10% for test set\n",
    "\n",
    "#### Perplexity\n",
    "The inverse probability that the model assign to the test set, normalized by the number of words, denoted as PP(W).\n",
    "\n",
    "Since the test set contains actual sentences from our data set, we expect the model to give a high probability to the sentences in it. \n",
    "\n",
    "Since $PP(W) \\propto \\frac{1}{Pr(W)}$, the higher the probability, the lower the perplexity.\n",
    "\n",
    "For a test set $W = w_1 \\dots W_N$:\n",
    "\n",
    "$$\n",
    "PP(W) = \\sqrt[N]{ \\frac{1}{Pr(w_1w_2\\dots w_N)}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\sqrt[N]{\\prod _{i=1} ^ N \\frac{1}{Pr(w_i | w_1 w_2\\dots w_{i-1})}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\prod _{i=1} ^ N  \\sqrt[N]{\\frac{1}{Pr(w_i | w_{i-n+1} \\dots w_{i-1})}}\n",
    "\\quad \\text{using a n-gram model}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "If the sentences in our test set is long, it is natural that our model will yield a smaller probability due to the longer sentences. \n",
    "Thus, we take the N-th root of the product normalize against the size of our test set.\n",
    "\n",
    "We can also view perplexity as the weighted average branching factor, which is the number of possible words that our model thinks can follow any word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
