{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expressions\n",
    "\n",
    "A formal language for specifying a set of text strings.\n",
    "\n",
    "A pattern that specify text search string to find in a corpus of text.\n",
    "\n",
    "For more info, refer to [Regular Expressions](../general-computing/regular_expression.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Errors\n",
    "\n",
    "1. False positive (Type I): Matching strings that we do not want to be matched\n",
    "2. False negative (Type II): Not matching strings we want to match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that we wish to find all occurence of `at`, regardless of capitalization, in the following corpus\n",
    "\n",
    "`All cats like to sleep at home. At noon, they wake up to eat`\n",
    "\n",
    "If we use the regular expression of `at`, we will get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All c(at)s like to sleep (at) home. At noon, they wake up to e(at)\n"
     ]
    }
   ],
   "source": [
    "def show_matches(exp, text):\n",
    "    for m in list(re.finditer(exp, text))[::-1]:\n",
    "        a, b = m.start(), m.end()\n",
    "        text = text[:b] + \")\" + text[b:]\n",
    "        text = text[:a] + \"(\" + text[a:]\n",
    "        \n",
    "    return text\n",
    "\n",
    "corpus = \"All cats like to sleep at home. At noon, they wake up to eat\"\n",
    "\n",
    "print(show_matches(r\"at\", corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which has both false positive when it matches `c(at)s` and `e(at)`, and false negative with `At`.\n",
    "\n",
    "The more correct expression would be `\\w[Aa][Tt]\\w`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All (cats) like to sleep at home. At noon, they wake up to eat\n"
     ]
    }
   ],
   "source": [
    "print(show_matches(r\"\\w[Aa][Tt]\\w\", corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing errors would involve two opposing efforts\n",
    "* Increase **accuracy/precision** $\\rightarrow$ Minimise false positive\n",
    "* Increase **coverage/recall** $\\rightarrow$ Minimise false negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many word\n",
    "\n",
    "I do uh main- mainly business data processing\n",
    "\n",
    "* Lemma: same stem, part of speech, rough word sense \n",
    "\n",
    "cat and cats = same lemma\n",
    "\n",
    "* Wordform: the full inflected surface form\n",
    "\n",
    "cat and cats = different wordforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "they lay back on the San Francisco grass and looked at the stars\n",
    "and their\n",
    "\n",
    "Type: Element of the vocabulary\n",
    "\n",
    "Token: Instance of that type in running text\n",
    "\n",
    "`San Francisco`: Should it be one word or 2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issues with Tokenization\n",
    "\n",
    "Findland's capital $\\rightarrow$ Finland, Finlands, Finland's\n",
    "\n",
    "what're $\\rightarrow$ What are\n",
    "\n",
    "Hewlett-Packet $\\rightarrow$ Hewlett Packard?\n",
    "\n",
    "This permeates through other languages\n",
    "\n",
    "French: L'ensemble\n",
    "\n",
    "German: Lebensvericherungsgesellschaftsangestellter\n",
    "\n",
    "Where it is compose of compound words, which we would want to split it into its components\n",
    "\n",
    "Chinese/Japanese have no spaces between words\n",
    "\n",
    "Multiple syllabaries are intermingled in Japanese: Hiragana, Katagana, Kanji\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Tokenization in Chinese\n",
    "\n",
    "Standard baseline segmentation algorithm: \n",
    "\n",
    "* Maximum/Greedy Matching\n",
    "\n",
    "### Maximum Matching\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Morphology?\n",
    "Listing all the different morphological variants of a word in a\n",
    "dictionary is inefficient.\n",
    "\n",
    "Affixes are productive; they apply to new words (e.g., fax and faxing).\n",
    "\n",
    "For morphologically complex languages like Turkish, it is\n",
    "impossible to list all morphological variants of every word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forms of Morphology\n",
    "\n",
    "### Inflectional\n",
    "* Combine a stem and an affix to form a word in the same class as stem\n",
    "* For syntactic function like agreement\n",
    "* e.g., -s to form plural form of a noun\n",
    "\n",
    "### Derivational\n",
    "* Combine a stem and an affix to form a word in a different class\n",
    "* Harder to predict the meaning of the derived form\n",
    "* e.g., -ation in computerize and computerization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out of Vocabulary\n",
    "New words are always being created\n",
    "\n",
    "However, we can apply morphology analysis to try to understand them even though we have never seen them before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repurposed Byte Pair Encoding (BPE)\n",
    "\n",
    "1. Find most frequent pairs in the string\n",
    "2. Update vocabulary with the pair with a new symbol\n",
    "3. Replace all pairs in the string with the new symbol\n",
    "4. Repeat up to k times, where k is a tunable parameter (corpus size dependant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "Convert text to a convenient standard form.\n",
    "\n",
    "`U.S.A` to `USA`\n",
    "\n",
    "Alternative: asymmetric expansion\n",
    "\n",
    "### Case folding\n",
    "\n",
    "### Lemmatization\n",
    "Reduce words to their base form\n",
    "\n",
    "### Penn Treebank Tokenization\n",
    "Seperate out clitics\n",
    "\n",
    "`don't` $\\rightarrow$ `do not`\n",
    "\n",
    "### Stemming\n",
    "Reduce terms to their stems\n",
    "\n",
    "Crude chopping of affixes\n",
    "\n",
    "* Language dependant\n",
    "\n",
    "#### Porter Stemmer\n",
    "Efficient stemming algorithm that is build on regular expressions\n",
    "\n",
    "Does not require a lexicon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Segmentation\n",
    "`!` and `?` are generally unambiguous as the end of the sentence.\n",
    "\n",
    "However, `.` is ambiguous, in the case of `U.S.A`, `Dr.`\n",
    "\n",
    "To solve this, we can use a decision tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spelling Errors\n",
    "\n",
    "1. Non-word error detection\n",
    "2. Isolated-word error correction\n",
    "\n",
    "### Spelling Error Pattterns\n",
    "Single-errors are the most common error in typewritten text\n",
    "\n",
    "* Insertion\n",
    "* Deletion\n",
    "* Substitution\n",
    "* Transposition\n",
    "\n",
    "### Candidate Generation\n",
    "Similar spelling\n",
    "* Small edit distance\n",
    "\n",
    "Similar pronounciation\n",
    "* Small edit distance to the pronounced word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noisy Channel Intuition\n",
    "\n",
    "We can model the corpus generation as through a noisy channel\n",
    "\n",
    "Our mission is to decode the noisy signal back into its original intentions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noisy Channel\n",
    "\n",
    "Given x as the misspelled word, we wish to find the correct word w\n",
    "\n",
    "$$\\hat w = argmax _{w\\in V} P(w|x)$$\n",
    "\n",
    "$$\\hat w = argmax _{w\\in V} \\frac{P(x| w)}{P(x)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edit Distance\n",
    "80% of errors are in edit distance 1\n",
    "Almost all errors are in  edit distance 2\n",
    "\n",
    "Allow insertion of space/hyphens\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
