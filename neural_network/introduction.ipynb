{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biological Neural Networks\n",
    "\n",
    "Consider the functioning of a biological neuron system\n",
    "\n",
    "It would consist of multiple neurons connected to each other, which would send signals to each other.\n",
    "The signal transmitted could either excite or inhibit the target neuron.\n",
    "\n",
    "And the output signal of the neuron can vary in magnitude based on the amount of excitation.\n",
    "\n",
    "The output is always non-negative as a neuron is incapable of producing a \"negative signal\"\n",
    "\n",
    "# Mathematical Representation\n",
    "Thus, we aim to represent the network as a mathematical function.\n",
    "\n",
    "Suppose that we model the neuron as having non-negative output, which models the biological neuron where it is fires a signal or not; where there is no \"negative\" signal.\n",
    "\n",
    "We denote the n inputs as $x_i$ for 1 to n\n",
    "\n",
    "A simple way to accumulate the signals at the neuron (where y is the signal received by the neuron) would simply be \n",
    "\n",
    "$$y =  \\sum ^ n _ 1 x_i$$\n",
    "\n",
    "However, this fails to capture the possible inhibitory nature of the input signal. And this model treats all input signals as having equal effect on the output, which is not the case in the biological neuron.\n",
    "\n",
    "Thus, to account for this behaviour, we would incorporate weights ($w_i$) to each of the input signal, resulting in \n",
    "\n",
    "$$y =  \\sum ^ n _ 1 x_i w_i$$\n",
    "\n",
    "Since we allow the weights to be negative, we can allow certain input signals to be treated as inhibitory to the neuron.\n",
    "\n",
    "Now, consider the case where there is many inhibitory signals. This may result in the resulting sum to be negative, which would violate our model that requires the output to be positive.\n",
    "\n",
    "Secondly, also consider the case where there is many excitory input signals, it may result in an unboundedly large output signal, which may not model real-life neurons which have a limit to how much output it can produce.\n",
    "\n",
    "Thus, to closely model these traits, we incorporate a \"squash\" function that aims to clamp the values of the output between 0 and 1. Squash functions includes step function and sigmoid function.\n",
    "\n",
    "We also incorporate a bias term into the squash function $\\varphi$ to act as the threshold for that specific neuron to fire.\n",
    "\n",
    "$$y =  \\varphi (\\sum ^ n _ 1 x_i w_i - b)$$\n",
    "\n",
    "Thus, this concludes our mathematical model of the neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ease of notation, in a particular neuron k,\n",
    "\n",
    "$$u_k = \\sum ^m_{j=1} w_{kj}x_j$$\n",
    "\n",
    "$$y_k = \\varphi (u_k+b_k)$$\n",
    "\n",
    "$x_1\\dots x_m$: the input signals\n",
    "\n",
    "$w_{k1}\\dots w_{km}$: the synaptic weights of neurons\n",
    "\n",
    "$u_k$: the linear combiner output due to input signals\n",
    "\n",
    "$b_k$: the bias\n",
    "\n",
    "$\\varphi (\\cdot)$: the activation function\n",
    "\n",
    "$y_k$: the output signal\n",
    "\n",
    "$v_k = u_k + b_k$: induced local field\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematical Simplification\n",
    "Alternatively, we can formulate the model as:\n",
    "\n",
    "$$v_k = \\sum ^m _{j=0} w_{kj}x_j$$\n",
    "\n",
    "$$y_k = \\varphi(v_k)$$\n",
    "\n",
    "Where we incorparate the bias as part of the input signal with $x_0 = 1$ and $w_{k0} = b_k$\n",
    "\n",
    "This allows us to compute the bias together with in the input rather than being a seperate step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Architectures\n",
    "\n",
    "## Layered Feedforward Networks\n",
    "\n",
    "* Nodes are separated into subsets called layers\n",
    "\n",
    "* There are no connections from layer $i$ to layer $j$ if $i > j$\n",
    "\n",
    "* Intra-layer connections (connections within the layer) may exist\n",
    "\n",
    "## Single-Layer Feedforward Networks\n",
    "\n",
    "The layer refers to the output layer, we do not count the input layer because there is no computation there.\n",
    "\n",
    "## Multi-layer Feedforward Networks\n",
    "\n",
    "Input of each layer is the output signal of the preceding layer only.\n",
    "\n",
    "There will be one or more hidden layers between the input and output layers, which contains hidden neurons that will provide useful computation before the signal reaches the output layer.\n",
    "\n",
    "## Recurrent Neural Networks\n",
    "\n",
    "As feedback is common in a biological nervous system, we can have networks where the neuron's output is used as inputs in the previous layer or even in the same neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning\n",
    "Neural network acquires knowledge via a **learning process**, where knowledge is stored as **synaptic weights**\n",
    "## Supervised Learning\n",
    "A \"teacher\" will provide the desired output for a given input and the neural network will adjust according to the error signal\n",
    "## Reinforcement Learning\n",
    "Network learns by making various actions, and a learning system will reward/penalized the network based on its actions. There is no explicit error signal\n",
    "## Unsupervised/Self-organized Learning\n",
    "Weights are purely adjusted based on input signals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benefits of Neural Networks\n",
    "\n",
    "## High computational power\n",
    "\n",
    "1. Generalization is possible, to produce reasonable outputs for inputs never encountered during training\n",
    "2. Has massively parallel distributed structure which allows for parallel computation\n",
    "\n",
    "## Properties\n",
    "\n",
    "1. Nonlinearity\n",
    "2. Adaptivity (plasticity): Able to adapt their weights to changes in environment\n",
    "3. Fault tolerance: If a neuron/synaptic link is damaged, the overall response may be unaffected because the information is stored in a distributed manner."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
