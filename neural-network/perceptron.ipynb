{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "def show_implementation(cls):\n",
    "    for line in inspect.getsourcelines(cls)[0]:\n",
    "        print(line, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron (Classification)\n",
    "\n",
    "Perceptron is a single layer neural network.\n",
    "\n",
    "It is designed to solve pattern recognition problem, where we classify a set of inputs $x_1 \\dots x_n$ into 2 classes, $C_1, C_2$.\n",
    "\n",
    "Squash function is always hard limiter $\\rightarrow$ output is either 1 or 0\n",
    "\n",
    "When induced local field $v(n) = w^T(n)(x(n) > 0$, it results in an output of $y(n) = 1$, and we group this as a class 1 classification. And it is a class 2 classification when $v(n) = w^T(n)(x(n) < 0$\n",
    "\n",
    "If induced local field is 0, then this is a decision boundary.\n",
    "\n",
    "## Geometric shape of Decision Boundary\n",
    "\n",
    "If m = 1, $w_1 x_1 + b = 0 \\rightarrow $ a point on the line\n",
    "\n",
    "If m = 2, $w_1 x_1 + w_2x_2 + b = 0 \\rightarrow $ a line on a 2D plane\n",
    "\n",
    "If m = 3, $w_1 x_1 + w_2x_2 + w_3x_3 + b = 0 \\rightarrow $ a plane on a 3D plane \n",
    "\n",
    "$w_1 x_1 + w_2x_2 \\dots w_mx_m + b = 0 \\rightarrow $ a hyperplane on a m-dimensional plane \n",
    "\n",
    "### Linearity of Decision Boundary\n",
    "Notice that for all values of m, the variables $x_i$ are all linear.\n",
    "This means that our decision boundary must be linear.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## How to choose weights\n",
    "The weights directly affect the decision boundary, thus we require a method to produce proper weights to solve the problem.\n",
    "\n",
    "The methods are:\n",
    "* Offline calculation of weights\n",
    "* Learning of weights\n",
    "\n",
    "### Offline weights calculation\n",
    "Suppose we are solving the problem of a OR gate, with the following logic table. Assume that we wish to solve this as a classification problem.\n",
    "\n",
    "|$$x_1$$|$$x_2$$|y  |\n",
    "|-----|-----|---|\n",
    "|  0  |  0  |0  |\n",
    "|  0  |  1  |1  |\n",
    "|  1  |  0  |1  |\n",
    "|  1  |  1  |1  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "\n",
    "xs = np.linspace(-0.5, 1.5, 100)\n",
    "ys = -xs + 0.5\n",
    "\n",
    "for (x,y) in pts:\n",
    "    plt.scatter([x], [y], color='red' if (x,y) == (0,0) else 'blue')\n",
    "plt.plot(xs, ys, '-k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "If we look at the graph, we notice that this is one possible decision boundary\n",
    "\n",
    "$x_2 = -x_1 + 0.5 \\Rightarrow x_1 + x_2 - 0.5 = 0$\n",
    "\n",
    "Thus, $w_0, w_1, w_2 = -0.5, 1, 1$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now, we will look at the implementation of the classical perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from module.perceptron import Perceptron\n",
    "from module.classical_perceptron import ClassicalPerceptron\n",
    "from module.activation_function import HardLimiter\n",
    "show_implementation(Perceptron)\n",
    "show_implementation(ClassicalPerceptron)\n",
    "show_implementation(HardLimiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = ClassicalPerceptron(2, [-0.5, 1, 1])\n",
    "\n",
    "inputs = [[0, 0],\n",
    "          [0, 1],\n",
    "          [1, 0],\n",
    "          [1, 1]]\n",
    "\n",
    "labels = [0, 1, 1, 1]\n",
    "\n",
    "for input, label in zip(inputs, labels):\n",
    "    print(f\"inputs: {input}, output: {p.get_output(input)}, correct label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this properly separates our inputs into 2 classes.\n",
    "\n",
    "Now, suppose we trying to solve the NOR problem instead. <a id=\"nor_gate\"></a>\n",
    "\n",
    "| $$x_1$$ | $$x_2$$ |y  |\n",
    "|:---:|:---:|:---:|\n",
    "|  0  |  0  |1  |\n",
    "|  0  |  1  |0  |\n",
    "|  1  |  0  |0  |\n",
    "|  1  |  1  |0  |\n",
    "\n",
    "Notice that the decision boundary will be the same as the previous one.\n",
    "\n",
    "However, we cannot reuse the weights because the signs will be wrong.\n",
    "\n",
    "The correct weights are $-x_1 - x_2 + 0.5 = 0$.\n",
    "\n",
    "However, for pattern recognition problem, we are not focused on the sign of the value of the output, but are simply aiming to separate the classes into different groups.\n",
    "\n",
    "**Linearly Separable**: when two classes can be separated by a line/plane/hyper-plane\n",
    "\n",
    "Notice that even though the examples are simple, once we start increasing the dimension, it becomes infeasible to find the boundary by hand.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Perceptron Learning\n",
    "Thus, to solve problems with higher dimension, we can turn to perceptron learning\n",
    "\n",
    "Suppose the classes $C_1$ and $C_2$ are linearly separable, there must exist some weight vector $w_0$ such that\n",
    "\n",
    "$$w_0^Tx>0 \\text{ for } \\forall x \\in C_1$$\n",
    "\n",
    "$$w_0^Tx<0 \\text{ for } \\forall x \\in C_2$$ \n",
    "\n",
    "We wish to find $w_0^T$ such that it can correctly classify the training set X.\n",
    "\n",
    "We can do so simply by **trial and error**.\n",
    "\n",
    "#### Learning Algorithm\n",
    "Suppose for a given weight, the output y is 0 while the correct label d is 1.\n",
    "\n",
    "We wish to find a new weight vector $w' = w + \\Delta w$ such that the induced local field is closer to 1, meaning $v'-v = w'^T x - w^T x = \\Delta w x > 0$.\n",
    "\n",
    "A simple vector that satisfy this relation is $x$, where $x^T x$ is always $\\geq 0$\n",
    "\n",
    "To prevent adjusting the weights \"too much\", we need a learning rate factor $\\eta$ which reduces the amount the weights are updated\n",
    "\n",
    "Thus, we need to update the weights to $w' = w + \\eta x$\n",
    "\n",
    "If we do the same for when the label is 0 and the output is 1, we would obtain the weight update to be $w' = w + \\eta x$\n",
    "\n",
    "To generalize these 2 cases, consider the error signal $e = d - y$.\n",
    "We obtain a general learning weight update value of $w' = w + \\eta e x$\n",
    "\n",
    "1. Initialize w(1) arbritrarily\n",
    "2. Feed a pattern x to the perceptron to produce a binary output y\n",
    "3. Compute error signal, $e(n) = d(n) - y(n) $\n",
    "4. Update weights, $w(n+1) = w(n) + \\eta e(n) x(n)$\n",
    "5. Repeat 2-4 until the weights no longer change\n",
    "\n",
    "**Note:** The weights are updated once for every input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from module.learning_perceptron import LearningPerceptron\n",
    "show_implementation(LearningPerceptron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = LearningPerceptron(2, HardLimiter(), [0.2,0.1,1])\n",
    "\n",
    "inputs = [[0, 0],\n",
    "          [0, 1],\n",
    "          [1, 0],\n",
    "          [1, 1]]\n",
    "\n",
    "labels = [0, 1, 1, 1]\n",
    "\n",
    "xs = np.linspace(-0.5, 1.5, 100)\n",
    "pts = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "\n",
    "for epoch in range(10):\n",
    "    prev_weights = p.weights\n",
    "\n",
    "    fig = plt.figure()\n",
    "    \n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    ys = (p.weights[1] * xs + p.weights[0]) / -p.weights[2]\n",
    "\n",
    "    for (x,y) in pts:\n",
    "        plt.scatter([x], [y], color='red' if (x,y) == (0,0) else 'blue')\n",
    "    \n",
    "    ax.set_title(f'Epoch {epoch}')\n",
    "\n",
    "    plt.plot(xs, ys, '-k')\n",
    "    plt.show()\n",
    "\n",
    "    for input, label in zip(inputs, labels):\n",
    "        p.train(input, label)\n",
    "\n",
    "    if np.array_equal(prev_weights, p.weights):\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proof of Convergence\n",
    "If the classes are linearly separable (there exists a $w_0$ which classifies the classes correctly), we can prove that this algorithm will converge to the solution in a fixed number of steps.\n",
    "\n",
    "Suppose w(1) = 0 and $\\eta = 1$.\n",
    "\n",
    "$w(n+1) = w(n) + e(n)x(n)$\n",
    "\n",
    "$w(2) = e(1)x(1)$\n",
    "\n",
    "$w(3) = e(1)x(1) + e(2)x(2)$\n",
    "\n",
    "$w(n+1) = \\sum _ {i=0} ^ n e(i)x(i)$\n",
    "\n",
    "Project both side along the direction of $w_0$\n",
    "\n",
    "$w_0 ^ T w(n+1) = \\sum _ {i=0} ^ n w_0 ^ T e(i)x(i)$\n",
    "\n",
    "Consider a certain term k in the summation\n",
    "\n",
    "$w_0 ^ T e(k)x(k) = e(k) w_0 ^ T x(k)$\n",
    "\n",
    "When $w_0^T x(k) > 0$, this means that the correct label is 1. Since this k-th term appeared in the summation, it must mean that the perceptron classified it wrongly, thus $d(k) = 1 , y(k) = 0$. \n",
    "\n",
    "Finally, we get $e(k) = 1 \\Rightarrow e(k) w_0 ^ T x(k)  > 0$\n",
    "\n",
    "When $w_0^T x(k) < 0 \\Rightarrow d(k) = 0 , y(k) = 1 \\Rightarrow e(k) = -1 \\Rightarrow e(k) w_0 ^ T x(k)  > 0$ \n",
    "\n",
    "Let $\\alpha = \\{\\min w_0^Tx(k)\\}$\n",
    "\n",
    "We obtain that $w_0 ^ T w(n+1) \\geq n \\alpha$\n",
    "\n",
    "By Cauchy-Schwarz inequality \"$||x||||y|| \\geq |x^Ty|$\", we get \n",
    "\n",
    "$$w_0 ^ T w(n+1) \\leq ||w_0|||w(n+1)||$$\n",
    "\n",
    "Thus, \n",
    "\n",
    "$$||w_0||||w(n+1)|| \\geq w_0 ^ T w(n+1) \\geq n\\alpha$$\n",
    "\n",
    "$$||w(n+1)|| \\geq \\frac{n\\alpha}{||w_0||}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, suppose we look at $||w(n+1)||^2$ instead.\n",
    "\n",
    "$||w(n+1)||^2 = w^T(n+1) w(n+1)$\n",
    "\n",
    "$= (w(n)+e(n)x(n))^T(w(n)+e(n)x(n))$\n",
    "\n",
    "$= w^T(n)w(n) + 2e(n)w^T(n)x(n) + e^2(n)x^T(n)x(n)$\n",
    "\n",
    "We now focus on $e(n)w^T(n)x(n)$.\n",
    "\n",
    "$e(n)w^T(n)x(n) = (d(n)-y(n)) w^T(n)x(n)$\n",
    "\n",
    "If $w^T(n)x(n) > 0 \\Rightarrow y(n) = 1, d(n) = 0 \\Rightarrow e(n)w^T(n)x(n) < 0$\n",
    "\n",
    "If $w^T(n)x(n) < 0 \\Rightarrow y(n) = 0, d(n) = 1 \\Rightarrow e(n)w^T(n)x(n) < 0$\n",
    "\n",
    "Thus, $||w(n+1)||^2  - ||w(n)||^2 =  w^T(n)w(n)  + 2e(n)w^T(n)x(n) + e^2(n)x^T(n)x(n) -  w^T(n)w(n) $\n",
    "\n",
    "$= 2e(n)w^T(n)x(n) + ||x(n)|| \\leq ||x(n)||^2$\n",
    "\n",
    "It follows that\n",
    "\n",
    "$$||w(2)||^2 - ||w(1)||^2 \\leq ||x(1)||^2$$\n",
    "\n",
    "$$||w(3)||^2 - ||w(2)||^2 \\leq ||x(2)||^2$$\n",
    "\n",
    "$$||w(n+1)||^2 - ||w(n)||^2 \\leq ||x(n)||^2$$\n",
    "\n",
    "Summing all the inequalities, we get $||w(n+1)||^2 - ||w(n)||^2  \\leq \\sum _{i=1} ^ n ||x(i)||^2$\n",
    "\n",
    "Let $\\beta = \\max {||x(i)||^2}$, we get $||w(n)||^2 \\leq n\\beta$\n",
    "\n",
    "Combined with the previous inequality we got $||w(n+1)|| \\geq \\frac{n\\alpha}{||w_0||} \\Rightarrow ||w(n+1)||^2 \\geq \\frac{n^2\\alpha^2}{||w_0||^2}$\n",
    "\n",
    "$$\\frac{n^2\\alpha^2}{||w_0||^2}\\leq ||w(n)||^2 \\leq n \\beta $$\n",
    "\n",
    "In this case, notice that if the weights change forever, this means that $n \\rightarrow \\infty$. However, because the LHS grows at the rate of $n^2$ while the RHS grows at the rate of n, the inequality will not hold. Therefore, n cannot approach infinity, and thus, the weights will converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impact of Learning Rate $\\eta$\n",
    "\n",
    "If $\\eta$ is too large, then learning will be quick for the current input, but it may cause it to \"unlearn\" what it has learnt in the previous input, leading to slower convergence.\n",
    "\n",
    "Similarly, if $\\eta$ is too small, it will learn the current input slowly, which also lead to a slower convergence.\n",
    "\n",
    "Thus, we need to choose an appropritate learning rate, which have to be determined by a problem-by-problem basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron (Regression)\n",
    "\n",
    "Suppose that we wish to find the relationship of multiple input variables $x_1\\dots x_m$ to a certain output variables $d$.\n",
    "And we have a set of observations of input-output data $\\{\\vec x(i), d(i)\\}$\n",
    "\n",
    "We can use a perceptron to solve this problem also.\n",
    "\n",
    "## Cost Function\n",
    "Suppose that our perceptron gave a model that has error e(i) compared each data point.\n",
    "\n",
    "A common cost function is the sum of squared errors, $E(w) = \\sum_{i=1} ^ n e (i)^2$\n",
    "\n",
    "This is used over $E(w) = \\sum_{i=1} ^ n |e (i)|$ because it has the added benefit of being differentiable.\n",
    "\n",
    "## Gradient Descent\n",
    "Thus, our goal is to find a weight vector $\\vec w$ such that the cost function $E(\\vec w)$ is minimized\n",
    "\n",
    "Notice that we require $\\nabla E(\\vec w) = 0$, where $\\nabla$ is the gradient operator $[\\frac{\\partial}{\\partial w_0}, \\dots \\frac{\\partial}{\\partial w_m}]^T$, since the rate of change must be 0 at the minimal point.\n",
    "\n",
    "Since solving $\\nabla E(\\vec w) = 0$ may be difficult, we use an iterative algorithm to find the minimal point instead.\n",
    "\n",
    "Suppose we have a certain $\\vec w(n)$. \n",
    "We can calculate the gradient of the error at this point with respect to each of the variables, $\\vec g(n) = \\nabla E(\\vec w(n))$. \n",
    "Since the gradient indicates the direction which the function increases the quickest, we would wish to move in the direction oppositve of the gradient to reduce the cost.\n",
    "\n",
    "Thus, intuitively, the update to the weights should be $\\Delta \\vec w(n) = -\\eta \\vec g(n)$ and \n",
    "\n",
    "$$ \\vec w(n+1) =  \\vec w(n) - \\eta \\vec g(n)$$\n",
    "\n",
    "---\n",
    "\n",
    "### Proof that using $\\nabla E(\\vec w(n))$ reduces error\n",
    "\n",
    "\n",
    "Consider $E(\\vec w(n+1))$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "E(\\vec w(n+1)) &= E(\\vec w(n)+\\Delta \\vec w(n)) &\\\\\n",
    "&\\approx E(\\vec w(n)) + \\frac{\\partial E}{\\partial \\vec w} \\Delta \\vec w(n) \\quad &\\text{ By Taylor Series approximation, assuming $\\Delta \\vec w(n)$ is small} \\\\\n",
    "&= E(\\vec w(n)) + (\\nabla E(\\vec w))^T \\Delta \\vec w(n) \\quad \\quad  &(\\nabla E(\\vec w))^T = \\frac{\\partial E}{\\partial \\vec w} \\quad \\text{by their definitions} \\\\\n",
    "&= E(\\vec w(n)) + (\\nabla E(\\vec w))^T ( - \\eta \\vec g(n))\\\\\n",
    "&= E(\\vec w(n)) - \\eta \\vec g(n)^T  \\vec g(n)\\\\\n",
    "&= E(\\vec w(n)) - \\eta ||\\vec g(n)|| <  E(\\vec w(n)) & \\text {QED}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "Note that since we require $\\Delta \\vec w(n)$ to be small, we need to small $\\eta$ so that our assumptions to be satisified.\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "1. Initialize $ \\vec w(0)$ arbitrarily\n",
    "2. Update  $ \\vec w(n+1) =  \\vec w(n) - \\eta \\vec g(n)$\n",
    "3. Repeat until convergence\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "### Definition\n",
    "Given observations of input data $\\{\\vec x_1, \\dots, \\vec x_m\\}$, output data $\\vec d$ where there are m variables and n observations, $\\vec x$ and $\\vec w$  will $m \\times 1$, $\\vec d$ and $\\vec y$ are $n \\times 1$.\n",
    "\n",
    "We wish to find $\\vec w$ such that\n",
    "\n",
    "$$E(\\vec w) = \\sum e^2 = \\sum (d_i - y_i)^2 \\text{ is minimized}$$\n",
    "\n",
    "where\n",
    "\n",
    "$$y_i =  \\vec w^T \\vec x_i$$\n",
    "\n",
    "\n",
    "### Optimality\n",
    "Since we are trying to find the weights that gives the minimum error, we wish to find $\\vec w(n)$ such that $\\frac{\\partial E}{\\partial \\vec w(n)} = \\vec 0$\n",
    "\n",
    "\n",
    "### Standard Least Square\n",
    "We can solve this linear regression directly.\n",
    "\n",
    "Rewrite \n",
    "\n",
    "$$y_i =  \\vec w^T \\vec x_i = \\vec x_i \\vec w^T$$\n",
    "\n",
    "$$\\vec y =   \\begin{pmatrix}\n",
    "x_1 ^T \\\\\n",
    "\\dots \\\\\n",
    "x_n ^T\n",
    "\\end{pmatrix} \\vec w = \\vec X \\vec w$$\n",
    "\n",
    "We call $\\vec X$ the regression matrix\n",
    "\n",
    "\n",
    "Observe that $E(\\vec w) = \\sum e^2 = \\vec e^T \\vec e$ and $\\vec e = \\vec d - \\vec y = \\vec d - \\vec X \\vec w$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial \\vec w} = \\frac{\\partial E}{\\partial \\vec e} \\frac{\\partial \\vec e}{\\partial \\vec w} = (2\\vec e ^T)(-\\vec X)= -2\\vec e^TX\n",
    "$$\n",
    "\n",
    "Set $\\frac{\\partial E}{\\partial w} = 0$\n",
    "\n",
    "$$\n",
    "-2\\vec e^T \\vec X = \\vec 0 = \\vec e^T \\vec X\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\vec e^T \\vec X = (\\vec d - X \\vec w)^T \\vec X =  \\vec d ^T\\vec X - \\vec w^T \\vec X^T \\vec X = \\vec 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Rightarrow \\vec d ^T \\vec X = \\vec w^T X^T \\vec X \\\\\n",
    "\\Rightarrow \\vec w^T =  \\vec d ^T \\vec X (\\vec X^T \\vec X)^{-1} \\\\\n",
    "\\Rightarrow \\vec w =  (\\vec X^T \\vec X)^{-1} \\vec X^T \\vec d\n",
    "$$\n",
    "\n",
    "### Example\n",
    "Suppose we wish to fit the following data, where there is only 1 variable and 5 observations\n",
    "\n",
    "|  x  |  d  |\n",
    "|:---:|:---:|\n",
    "|  3  | 3.5 |\n",
    "| 3.5 | 2.5 |\n",
    "|  5  |  2  |\n",
    "| 5.5 |  1  |\n",
    "|  6  |  1  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts = [([3], 3.5), ([3.5], 2.5), ([5], 2), ([5.5], 1), ([6], 1)]\n",
    "\n",
    "f, ax = plt.subplots(1)\n",
    "xdata, ydata = zip(*pts)\n",
    "ax.scatter(xdata, ydata)\n",
    "ax.set_ylim(ymin=0, ymax=10)\n",
    "ax.set_xlim(xmin=0, xmax=10)\n",
    "ax.set_ylabel('Output (d)')\n",
    "ax.set_xlabel('Input (x)')\n",
    "\n",
    "bias = np.ones((len(pts), 1))\n",
    "X = np.append(bias, np.array(xdata), axis=1)\n",
    "d = np.array(ydata)\n",
    "w = np.linalg.inv(X.T @ X) @ X.T @ d\n",
    "\n",
    "xs = np.linspace(0, 10, 100)\n",
    "ys = w[1] * xs + w[0]\n",
    "plt.plot(xs, ys, '-r')\n",
    "print(w)\n",
    "plt.show(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the line does resemble the best fit line.\n",
    "\n",
    "### Issues\n",
    "Notice that the regression matrix is $n \\times (m+1)$.\n",
    "\n",
    "This means that to compute $w$, we would need to store $n(m+1)$ entries in memory.\n",
    "\n",
    "If n and m is large, this becomes infeasible.\n",
    "\n",
    "---\n",
    "\n",
    "### Perceptron\n",
    "Suppose we use a perceptron to solve the problem.\n",
    "\n",
    "We would need to replace the activation function (hard limiter) with a linear function, so that we can get a range of values as the output rather than it being binary.\n",
    "\n",
    "Suppose our cost function is $E(\\vec w) = \\frac{1}{2} e^2(n)$, where $e(n) = d(n) - \\vec x^T(n) \\vec w(n)$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial \\vec w} = \\frac{\\partial E}{\\partial \\vec e} \\frac{\\partial \\vec e}{\\partial \\vec w} = e(n)(-\\vec x^T(n))\n",
    "$$\n",
    "$$\n",
    "g(n) = (\\frac{\\partial E(w)}{\\partial \\vec w(n)})^T = - e(n)\\vec x(n)\n",
    "$$\n",
    "\n",
    "Thus\n",
    "\n",
    "$$\n",
    "w(n+1) = w(n) + \\eta e(n)\\vec x(n)\n",
    "$$\n",
    "\n",
    "by gradient descent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from module.regression_perceptron import RegressionPerceptron\n",
    "from module.activation_function import Linear\n",
    "show_implementation(RegressionPerceptron)\n",
    "show_implementation(Linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts = [([3], 3.5), ([3.5], 2.5), ([5], 2), ([5.5], 1), ([6], 1)]\n",
    "           \n",
    "perceptron = RegressionPerceptron(1, weights=[2,0], learning_rate=0.01)\n",
    "\n",
    "f, ax = plt.subplots(1)\n",
    "xdata, ydata = zip(*pts)\n",
    "ax.scatter(xdata, ydata)\n",
    "ax.set_ylim(ymin=0, ymax=10)\n",
    "ax.set_xlim(xmin=0, xmax=10)\n",
    "ax.set_ylabel('Output (d)')\n",
    "ax.set_xlabel('Input (x)')\n",
    "\n",
    "perceptron.train_epoch(xdata, ydata,epochs=1000)\n",
    "w = perceptron.weights\n",
    "\n",
    "xs = np.linspace(0, 10, 100)\n",
    "ys = w[1] * xs + w[0]\n",
    "plt.plot(xs, ys, '-r')\n",
    "print(w)\n",
    "plt.show(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is evident that the gradient descent variant yield similar results.\n",
    "\n",
    "#### Speedup\n",
    "Since we required $\\eta$ to be small, the learning rate will be slow.\n",
    "\n",
    "But if we modified our $\\Delta w(k)$ from\n",
    "\n",
    "$$\n",
    "\\Delta w(k) = \\eta \\delta x\n",
    "$$\n",
    "\n",
    "to\n",
    "\n",
    "$$\n",
    "\\Delta w(k) = \\alpha \\Delta w(k-1) + \\eta \\delta x\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is the momentum constant/forgetting factor and $\\alpha \\Delta w(k-1)$ is the momentum term.\n",
    "\n",
    "This new $\\Delta w$ have the property that when consecutive $\\Delta w(k)$ are of the same sign, the magnitude of $\\Delta w$ increases.\n",
    "This means that learning will be faster as we pick up \"momentum\" when going down a steady gradient.\n",
    "\n",
    "And when the signs are $\\Delta w(k)$ are different, it will reduce the speed of learning, allowing the network to settle at a minimum.\n",
    "\n",
    "**Footnote**: We can actually give different learning rates for each layer, each denoted by $\\eta ^{(i)}$. However, this is unconventional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Termination\n",
    "Notice that unlike in the classification problem, it is unlikely that the minimum error is 0 (unless the points lie in a line), thus the weights will never converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
